{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estructura que vamos a implementar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generador de posiciones de vehiculos\n",
    "Revisar la función Treading.Thread, genera la misma función de python varias veces a la vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui deberíamos poner el generador que tenemos, como idea, podemos guardar los mensajes en un bucket o mandarlos a pupsug, \n",
    "# revisar la función que genera varios posiciones random de diferentes vehículos como en el postwork de Javi.\n",
    "# En el trabajo final, esto lo puede hacer un VM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generador de posiciones de individuos\n",
    "Revisar la función Treading.Thread, genera la misma función de python varias veces a la vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer otro generador, que tomando las posiciones de los kml´s y añadiendo  +/- x puntos, nos de posiciones \n",
    "# random de los usuarios. Podemos tambien guardarlos en otro bucket para posteriormente leerlos de ahí o mandarlos a pupsug.\n",
    "# En el trabajo final, esto lo puede hacer un VM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colas de mensajes. Tendremos así dos colas de PupSub que leerán los mensajes de los dos buckets: vehículos y clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\" Part 01: Read data from PubSub. \"\"\"\n",
    "\n",
    "        data1 = (\n",
    "            p\n",
    "                | \"Read From PubSub\" >> # leemos del bucket de vehiculos\n",
    "                | \"Parse JSON messages\" >> #Tenemos que ver cual es la estructura del mensaje y transformarlo a nuetra conveniencia\n",
    "        )\n",
    "\n",
    "        data2 = (\n",
    "            p\n",
    "                | \"Read From PubSub\" >> # leemos del bucket de clientes\n",
    "                | \"Parse JSON messages\" >> #Tenemos que ver cual es la estructura del mensaje y transformarlo a nuetra conveniencia\n",
    "        )\n",
    "\n",
    "        \"\"\" Part 02: Get the aggregated data of the vehicle within the section. \"\"\"\n",
    "\n",
    "        data1, data2, processed_data = (\n",
    "            \n",
    "             \n",
    "                | \"tenemos que leer los datos de ambos topics\" >> #ToDo: Complete this section\n",
    "                | \"Cramos una ventana temporal donde\" >> # Tendremos las posiciones de los vehiculos y de los clientes\n",
    "                | \"Creamos una funcion (Dofn)\" >> # Donde comparemos las distancias entre ambos\n",
    "                # si esta distancia es > que x match, sino, no match.\n",
    "                | \"Tendremos que guardar los match en bigquery\" >> \n",
    "                |\"Ver donde guardamos los no match si en bigquery o en otro bucket sin más\"\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leeremos de bigquery los resultados match y los mostramos con streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generador ofertas Vehículos\n",
    "def read_kml(oferta, bucket_name, file_id, project_id, topic_name):\n",
    "    pubsub_class = PubSubMessages(project_id, topic_name)\n",
    "\n",
    "    kml_file = os.path.join(DOWNLOAD_FOLDER, f'{file_id}.kml')\n",
    "    download_blob(bucket_name, f'{file_id}.kml', kml_file)\n",
    "\n",
    "    data = {\"id_oferta\": [], \"punto\": [], \"latitude\": [], \"longitude\": []}\n",
    "    datos_longitude = []\n",
    "    datos_latitude = []\n",
    "\n",
    "    with open(kml_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        kml_data = file.read()\n",
    "\n",
    "    root = ET.fromstring(kml_data)\n",
    "    coords = root.find(\".//{http://www.opengis.net/kml/2.2}LineString/{http://www.opengis.net/kml/2.2}coordinates\")\n",
    "\n",
    "    if coords is not None:\n",
    "        coords_str = coords.text\n",
    "        coords_list = [tuple(map(float, _.split(','))) for _ in coords_str.split()]\n",
    "\n",
    "        for _, coords in enumerate(coords_list):\n",
    "            data[\"id_oferta\"] = oferta\n",
    "            data[\"punto\"] = _ + 1\n",
    "            data[\"latitude\"] = coords[1]\n",
    "            data[\"longitude\"] = coords[0]\n",
    "            datos_latitude.append(coords[1])\n",
    "            datos_longitude.append(coords[0])\n",
    "            print(data)\n",
    "            pubsub_class.publish_message(data)\n",
    "            time.sleep(1)\n",
    "\n",
    "    return datos_longitude, datos_latitude\n",
    "\n",
    "\n",
    "def get_coords_finales(bucket_name, DOWNLOAD_FOLDER):\n",
    "    latitudes_finales = []\n",
    "    longitudes_finales = []\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    blobs = bucket.list_blobs()\n",
    "\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith(\".kml\"):\n",
    "            kml_file = os.path.join(DOWNLOAD_FOLDER, blob.name)\n",
    "\n",
    "            with open(kml_file, \"wb\") as file:\n",
    "                blob.download_to_file(file)\n",
    "\n",
    "            with open(kml_file, \"r\", encoding=\"utf-8\") as archivo:\n",
    "                datos_kml = archivo.read()\n",
    "\n",
    "            raiz = ET.fromstring(datos_kml)\n",
    "            coordenadas = raiz.findall(\".//{http://www.opengis.net/kml/2.2}coordinates\")\n",
    "\n",
    "            if coordenadas:\n",
    "                for coordenada_str in coordenadas:\n",
    "                    coords_lista = [tuple(map(float, _.split(','))) for _ in coordenada_str.text.split()]\n",
    "                    for coords in coords_lista:\n",
    "                        longitudes_finales.append(coords[0])\n",
    "                        latitudes_finales.append(coords[1])\n",
    "\n",
    "    return latitudes_finales, longitudes_finales\n",
    "\n",
    "def gen_ofertas(num_ofertas, project_id, topic_name, bucket_name):\n",
    "    datos_longitude_total = []\n",
    "    datos_latitude_total = []\n",
    "\n",
    "    for i in range(1, num_ofertas + 1):\n",
    "        file_id = random.randint(1, 27)\n",
    "        datos_longitude, datos_latitude = read_kml(\n",
    "            oferta=i, bucket_name=bucket_name, file_id=file_id, project_id=project_id, topic_name=topic_name)\n",
    "        datos_longitude_total = datos_longitude\n",
    "        datos_latitude_total = datos_latitude\n",
    "\n",
    "    return datos_longitude_total, datos_latitude_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generador demandas solicitantes\n",
    "\n",
    "def gen_solicitudes(num_solicitudes, project_id, topic_name, datos_latitude_total, datos_longitude_total, latitudes_finales, longitudes_finales):\n",
    "\n",
    "    pubsub_class = PubSubMessages(project_id, topic_name)\n",
    "\n",
    "    data_solicitud = {\"id_solicitud\": [], \"latitude\": [], \"longitude\": [], \"latitude_destino\": [], \"longitude_destino\": []}\n",
    "    for i in range(1, num_solicitudes + 1):\n",
    "        data_solicitud['id_solicitud'] = i\n",
    "        data_solicitud['latitude'] = random.choice(datos_latitude_total)\n",
    "        data_solicitud['longitude'] = random.choice(datos_longitude_total)\n",
    "        data_solicitud['latitude_destino'] = random.choice(latitudes_finales)\n",
    "        data_solicitud['longitude_destino'] = random.choice(longitudes_finales)\n",
    "        print(data_solicitud)\n",
    "        pubsub_class.publish_message(data_solicitud)\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import google.cloud\n",
    "from google.cloud import pubsub_v1\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import Blob\n",
    "import xml.etree.ElementTree as ET\n",
    "import argparse\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un pipeline de vehiculos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: apache-beam\n",
      "Version: 2.51.0\n",
      "Summary: Apache Beam SDK for Python\n",
      "Home-page: https://beam.apache.org\n",
      "Author: Apache Software Foundation\n",
      "Author-email: dev@beam.apache.org\n",
      "License: Apache License, Version 2.0\n",
      "Location: c:\\users\\josan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages\n",
      "Requires: cloudpickle, crcmod, dill, fastavro, fasteners, grpcio, hdfs, httplib2, js2py, numpy, objsize, orjson, packaging, proto-plus, protobuf, pyarrow, pydot, pymongo, python-dateutil, pytz, regex, requests, typing-extensions, zstandard\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show apache_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Libraries\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      3\u001b[0m datos_demandas \u001b[38;5;241m=\u001b[39m get_coords_finalesClientes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/josan/Documents/GitHub/DATA_PROJECT_2/Ejecutable/coordenadas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m datos_ofertas \u001b[38;5;241m=\u001b[39m \u001b[43mget_coords_finalesVehiculos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/josan/Documents/GitHub/DATA_PROJECT_2/Ejecutable/coordenadas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransformarMensaje\u001b[39;00m(beam\u001b[38;5;241m.\u001b[39mDoFn):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, element):\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# Extraer la latitud y longitud del elemento\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m, in \u001b[0;36mget_coords_finalesVehiculos\u001b[1;34m(local_folder)\u001b[0m\n\u001b[0;32m     26\u001b[0m                     data1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m coords[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     27\u001b[0m                     datos_ofertas\u001b[38;5;241m.\u001b[39mappend(data1)\n\u001b[1;32m---> 28\u001b[0m                     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datos_ofertas\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "datos_demandas = get_coords_finalesClientes(\"C:/Users/josan/Documents/GitHub/DATA_PROJECT_2/Ejecutable/coordenadas\")\n",
    "datos_ofertas = get_coords_finalesVehiculos(\"C:/Users/josan/Documents/GitHub/DATA_PROJECT_2/Ejecutable/coordenadas\")\n",
    "\n",
    "class TransformarMensaje(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        # Extraer la latitud y longitud del elemento\n",
    "        latitude = element['latitude']\n",
    "        longitude = element['longitude']\n",
    "\n",
    "        # Crear la tupla de dos elementos\n",
    "        tupla_lat_long = (latitude, longitude)\n",
    "\n",
    "        # Devolver el mensaje transformado\n",
    "        return [(tupla_lat_long, element)]\n",
    "\n",
    "\n",
    "\n",
    "# Crear un pipeline de Apache Beam\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    # Leer los datos_ofertas usando la transformación Create\n",
    "    ofertas_vehiculos= ( \n",
    "        p   \n",
    "        | \"Create PCollection\" >> beam.Create(datos_ofertas)# datos\n",
    "        | \"Transformamos los datos con el Dofn\">> beam.ParDo(TransformarMensaje())\n",
    "        # | \"Agrupamos por clave\" >> beam.GroupByKey() no\n",
    "        # Agrupamos por id vehiculo y por posición\n",
    "        \n",
    "    )\n",
    "    demandas_vehiculos= ( \n",
    "        p   \n",
    "        | \"Create PCollection2\" >> beam.Create(datos_demandas)# datos\n",
    "        | \"Transformamos los datos con el Dofn2\" >> beam.ParDo(TransformarMensaje())\n",
    "        # | \"Agrupamos por clave2\" >> beam.GroupByKey()\n",
    "        \n",
    "    )\n",
    "    combined_pcollection = (\n",
    "        (ofertas_vehiculos, demandas_vehiculos)\n",
    "        | \"Creamos una tercera Pcollection a partir de las anteriores\">> beam.CoGroupByKey()\n",
    "        | \"Agrupamos por key\" >> beam.GroupByKey() \n",
    "        | \"Imprimimos\" >>beam.Map(print) \n",
    "        )\n",
    "\n",
    "\n",
    "#Agrupar mensaje vehículo por id y comparar sus posiciones con posición vehiculo\n",
    "   \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el cogroup by key agrupamos todos los datos de los dos subconjuntos en en una lista de datos que tienen la misma clave.\n",
    "Tengo que probar tambien:\n",
    " combined_pcollection = (ofertas_vehiculos, demandas_vehiculos) | beam.CombinePerKey(lambda x, y: (sum(x), sum(y)))\n",
    "Y si no nos genera el resultado esperado, intentar crar un Dofn que nos dé el resultado que esperamos.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
